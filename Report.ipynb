{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I've provided complete code for starting a unity environment and training an agent to pick up yellow banana in the environment using deep Q network (DQN). \n",
    "\n",
    "The code contains three major part : 1. The neural network model used by the DQN agent.  2. The DQN agent and its replay buffer. 3. Training code for the agent to interact with the environment and lean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Network Model\n",
    "\n",
    "Here we use a multi-layer perceptron as the approximator for the state-action function for the DQN agent. The network has three fully connected layers and uses rectified linear unit as its acvitation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Network\n",
    "\n",
    "I chose to use Deep Q Network as the learning algorithm. The DQN has a multi-layer perceptron as function approximator and uses fixed Q-target technique to stablize learning process. It's implemented as follows : I created two Q networks : target Q Network, local Q Network. During the training process, I only let local Q network update its parameter. After parameters for local Q network were updated, I then soft update the target Q network using the formula   \n",
    "\n",
    "$$\\text{tau*local parameters + (1.0-tau)*target parameters}$$   \n",
    "\n",
    "Where `tau` is a small scalar value to control the rate of soft-update. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Process\n",
    "\n",
    "The training process is as follows :   \n",
    "1. The agent acts\n",
    "2. The environment changes its state and the agent gets a reward\n",
    "3. The agent commits state, action, reward to replay buffer\n",
    "4. Once the replay buffer is filled, the agent chooses actions from replay buffer using an epsilon-greedy policy.\n",
    "\n",
    "The hyperparameters were selected mostly through a trial-and-error process. \n",
    "\n",
    "Here's the result of training process:\n",
    "![training result](download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further improvement\n",
    "\n",
    "The training process could popssible be further improved by adopting a prioritized replay strategy, giving experiences that lead to largest temporal difference higher chance to be selected for replay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
